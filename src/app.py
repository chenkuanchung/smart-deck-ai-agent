# src/app.py
import streamlit as st
import os

# è¼‰å…¥æˆ‘å€‘çš„æ¨¡çµ„
from src.config import Config
from src.tools.rag import ingest_file, rag_tool
from src.tools.search import search_tool
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage

# --- 1. åˆå§‹åŒ–è¨­å®š ---
st.set_page_config(page_title="Smart Deck Agent", page_icon="ğŸ“Š", layout="wide")

# ç¢ºä¿ç’°å¢ƒè®Šæ•¸å·²è¼‰å…¥
try:
    Config.validate()
except Exception as e:
    st.error(f"ç’°å¢ƒè¨­å®šéŒ¯èª¤: {e}")
    st.stop()

# --- 2. åˆå§‹åŒ– LLM èˆ‡å·¥å…·ç¶å®š (çœŸæ­£çš„ AI å¤§è…¦) ---

# å®šç¾©å·¥å…·ç®±ï¼šå‘Šè¨´ Gemini å®ƒæœ‰å“ªäº›è¶…èƒ½åŠ›
tools = [rag_tool, search_tool]

# å»ºç«‹å·¥å…·å°ç…§è¡¨ (ç”¨æ–¼ç¨‹å¼åŸ·è¡Œ)
tool_map = {
    "read_knowledge_base": rag_tool,
    "google_search": search_tool
}

# åˆå§‹åŒ– LLM
# é€™è£¡å»ºè­°ä½¿ç”¨ MODEL_SMART (Pro) æˆ–è‡³å°‘ Flashï¼Œå› ç‚º Function Calling éœ€è¦è¼ƒå¥½çš„æ¨ç†èƒ½åŠ›
llm = ChatGoogleGenerativeAI(
    model=Config.MODEL_FAST,  # å»ºè­°ç”¨ Flash ä¿æŒé€Ÿåº¦ï¼Œè‹¥ç™¼ç¾åˆ¤æ–·ä¸æº–å¯æ”¹ç”¨ SMART
    google_api_key=Config.GOOGLE_API_KEY,
    temperature=0.7
)

# [é—œéµæŠ€è¡“] Bind Tools: å°‡å·¥å…·ç¶å®šçµ¦æ¨¡å‹
# é€™ä¸€æ­¥ä¹‹å¾Œï¼ŒGemini å°±çŸ¥é“è‡ªå·±å¯ä»¥å‘¼å«é€™äº›å‡½å¼äº†
llm_with_tools = llm.bind_tools(tools)

# --- 3. Session State ç®¡ç† ---
if "messages" not in st.session_state:
    st.session_state.messages = []

if "uploaded_files" not in st.session_state:
    st.session_state.uploaded_files = []

# --- 4. å´é‚Šæ¬„ (ä¸Šå‚³å€) ---
with st.sidebar:
    st.header("ğŸ“‚ è³‡æ–™ä¾†æº")
    st.caption("è«‹å…ˆä¸Šå‚³æ–‡ä»¶ï¼Œè®“ AI æ“æœ‰å…§éƒ¨çŸ¥è­˜ã€‚")
    
    uploaded_file = st.file_uploader("é¸æ“‡æª”æ¡ˆ (PDF/TXT)", type=["pdf", "txt"])
    
    if uploaded_file and uploaded_file.name not in st.session_state.uploaded_files:
        with st.spinner("æ­£åœ¨è®€å–ä¸¦å‘é‡åŒ–æ–‡ä»¶..."):
            temp_path = os.path.join(os.getcwd(), uploaded_file.name)
            with open(temp_path, "wb") as f:
                f.write(uploaded_file.getbuffer())
            
            # å‘¼å« RAG Ingest
            result = ingest_file(temp_path)
            
            if "æˆåŠŸ" in result:
                st.success(result)
                st.session_state.uploaded_files.append(uploaded_file.name)
            else:
                st.error(result)

    st.divider()
    
    st.header("ğŸš€ ç”Ÿæˆè¡Œå‹•")
    if st.button("âœ¨ ç”Ÿæˆ PPT ç°¡å ±", type="primary"):
        st.info("åŠŸèƒ½é–‹ç™¼ä¸­... (é€™è£¡å°‡ä¸²æ¥ Manager Agent é€²è¡Œå¤§ç¶±è¦åŠƒèˆ‡ç”Ÿæˆ)")

# --- 5. ä¸»èŠå¤©ä»‹é¢ (æ™ºæ…§åˆ¤æ–·æ ¸å¿ƒ) ---
st.title("ğŸ’¬ Smart Deck Agent")
st.caption("å·²å•Ÿç”¨æ™ºæ…§å·¥å…·åˆ¤æ–·æ¨¡å¼ (Function Calling)ã€‚è«‹ç›´æ¥è¼¸å…¥æ‚¨çš„éœ€æ±‚ã€‚")

# é¡¯ç¤ºæ­·å²è¨Šæ¯
for msg in st.session_state.messages:
    if isinstance(msg, HumanMessage):
        with st.chat_message("user"):
            st.markdown(msg.content)
    elif isinstance(msg, AIMessage):
        # å¦‚æœé€™æ¢ AI è¨Šæ¯æœ‰å…§å®¹ (ä¸æ˜¯ç´”å·¥å…·å‘¼å«)ï¼Œå°±é¡¯ç¤ºå‡ºä¾†
        if msg.content:
            with st.chat_message("assistant"):
                st.markdown(msg.content)
    # ToolMessage (å·¥å…·å›å‚³çš„ Raw Data) æˆ‘å€‘é¸æ“‡ä¸ç›´æ¥é¡¯ç¤ºï¼Œä¿æŒä»‹é¢ä¹¾æ·¨

# è™•ç†ä½¿ç”¨è€…è¼¸å…¥
if prompt := st.chat_input("è¼¸å…¥è¨Šæ¯... (ä¾‹å¦‚ï¼šå¹«æˆ‘ç¸½çµé€™ä»½å ±å‘Šã€æŸ¥ä¸€ä¸‹æœ€æ–° AI æ–°è)"):
    
    # 1. ç´€éŒ„ä¸¦é¡¯ç¤ºä½¿ç”¨è€…è¨Šæ¯
    st.session_state.messages.append(HumanMessage(content=prompt))
    with st.chat_message("user"):
        st.markdown(prompt)

    # 2. AI æ€è€ƒè¿´åœˆ
    with st.chat_message("assistant"):
        # å»ºç«‹ä¸€å€‹ä½”ä½ç¬¦ä¾†é¡¯ç¤ºç‹€æ…‹ (ä¾‹å¦‚ï¼šæ­£åœ¨æœå°‹...)
        status_container = st.empty()
        
        with st.spinner("æ­£åœ¨æ€è€ƒä¸‹ä¸€æ­¥..."):
            # Step A: å°‡å®Œæ•´å°è©±ç´€éŒ„ä¸Ÿçµ¦ Gemini
            response = llm_with_tools.invoke(st.session_state.messages)
            
            # å­˜å…¥å°è©±ç´€éŒ„ (åŒ…å« AI çš„æ€è€ƒèˆ‡å¯èƒ½çš„å·¥å…·å‘¼å«åƒæ•¸)
            st.session_state.messages.append(response)

            # Step B: åˆ¤æ–· Gemini æ˜¯å¦æ±ºå®šè¦ä½¿ç”¨å·¥å…·?
            if response.tool_calls:
                # é€™è£¡å¯èƒ½æœƒæœ‰å¤šæ¬¡å·¥å…·å‘¼å« (Parallel Function Calling)
                for tool_call in response.tool_calls:
                    tool_name = tool_call["name"]
                    tool_args = tool_call["args"]
                    call_id = tool_call["id"]
                    
                    # --- UX å„ªåŒ–ï¼šå‘Šè¨´ä½¿ç”¨è€…ç¾åœ¨ç™¼ç”Ÿä»€éº¼äº‹ ---
                    if tool_name == "read_knowledge_base":
                        status_container.info(f"ğŸ“š æ­£åœ¨é–±è®€çŸ¥è­˜åº«... (æŸ¥è©¢: {tool_args.get('query', '...')})")
                    elif tool_name == "google_search":
                        status_container.info(f"ğŸŒ æ­£åœ¨æœå°‹ç¶²è·¯... (é—œéµå­—: {tool_args.get('query', '...')})")
                    
                    # --- åŸ·è¡Œå·¥å…· ---
                    selected_tool = tool_map.get(tool_name)
                    tool_output = "å·¥å…·åŸ·è¡Œå¤±æ•—"
                    
                    if selected_tool:
                        try:
                            # é‡å°æˆ‘å€‘å®šç¾©çš„ Toolï¼Œé€šå¸¸åªæœ‰ä¸€å€‹åƒæ•¸ (query)
                            # é€™è£¡ç›´æ¥å–åƒæ•¸å€¼å‚³å…¥
                            arg_value = next(iter(tool_args.values())) if tool_args else ""
                            tool_output = selected_tool.invoke(arg_value)
                        except Exception as e:
                            tool_output = f"Error: {e}"
                    
                    # --- å°‡å·¥å…·çµæœå›å‚³çµ¦ AI ---
                    # é€™æ˜¯é—œéµï¼šæˆ‘å€‘æŠŠæŸ¥åˆ°çš„è³‡æ–™åŒ…æˆ ToolMessage å¡å›çµ¦ Gemini
                    st.session_state.messages.append(
                        ToolMessage(content=str(tool_output), tool_call_id=call_id, name=tool_name)
                    )
                
                # Step C: è®“ Gemini æ ¹æ“šæŸ¥åˆ°çš„è³‡æ–™ï¼Œç”Ÿæˆæœ€çµ‚å›ç­”
                final_response = llm_with_tools.invoke(st.session_state.messages)
                
                # é¡¯ç¤ºæœ€çµ‚å›ç­”
                st.markdown(final_response.content)
                st.session_state.messages.append(final_response)
                
                # æ¸…é™¤ç‹€æ…‹æç¤º
                status_container.empty()
                
            else:
                # å¦‚æœ Gemini è¦ºå¾—ä¸ç”¨æŸ¥è³‡æ–™ (ä¾‹å¦‚åªæ˜¯æ‰“æ‹›å‘¼)ï¼Œç›´æ¥é¡¯ç¤ºå›æ‡‰
                st.markdown(response.content)