# src/app.py
import streamlit as st
import os
import sys

# è·¯å¾‘ä¿®æ­£
current_file_path = os.path.abspath(__file__)
src_dir = os.path.dirname(current_file_path)
project_root = os.path.dirname(src_dir)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from src.config import Config
from src.tools.rag import ingest_file, rag_tool, reset_vector_store, remove_file_from_db
from src.tools.search import search_tool
from src.graph import agent_workflow
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, SystemMessage

# --- [UX å„ªåŒ–] å·¥å…·åç¨±å°ç…§è¡¨ ---
TOOL_DISPLAY_NAMES = {
    "google_search": "ğŸŒ æ­£åœ¨æœå°‹ç¶²è·¯... (Web Research)",
    "read_knowledge_base": "ğŸ“š æ­£åœ¨åˆ†æå…§éƒ¨æ–‡ä»¶ (Document Analysis)"
}

# --- System Prompt (æœ€å¼·åˆé«”ç‰ˆï¼šæ·±åº¦æœå°‹ + æ„åœ–å°å‘) ---
SYSTEM_PROMPT_TEMPLATE = """
ä½ ä¸å†åªæ˜¯ä¸€å€‹åŠ©ç†ï¼Œä½ æ˜¯ **Smart Deck åœ˜éšŠçš„ã€Œé¦–å¸­ç ”ç©¶åˆ†æå¸« (Lead Research Analyst)ã€**ã€‚
ä½ çš„ç›®æ¨™æ˜¯ç‚ºä½¿ç”¨è€…çš„ç°¡å ±æä¾›**ã€Œé«˜è³‡è¨Šå¯†åº¦ (High Information Density)ã€**çš„ç´ æï¼Œè€Œéæ³›æ³›è€Œè«‡ã€‚

### ç•¶å‰ç’°å¢ƒç‹€æ…‹ï¼š
- **å·²ä¸Šå‚³æ–‡ä»¶æ•¸é‡**ï¼š{file_count} ä»½
- **æ–‡ä»¶åˆ—è¡¨**ï¼š{file_names}

### ä½ çš„æ ¸å¿ƒæ±ºç­–é‚è¼¯ (Core Protocol)ï¼š

1. **ç‹€æ³ Aï¼šä½¿ç”¨è€…è¼¸å…¥ã€Œåè©ã€æˆ–ã€ŒçŸ­èªã€ï¼ˆä¾‹å¦‚ï¼š"é‡å­åŠ›å­¸"ã€"ç”Ÿæˆå¼ AI"ï¼‰**
   - **è§£è®€**ï¼šä½¿ç”¨è€…æ­£åœ¨**ã€Œè¨­å®šç°¡å ±ä¸»é¡Œã€**ã€‚
   - **è¡Œå‹•**ï¼šä½ å¿…é ˆç«‹åˆ»å•Ÿå‹•**æ·±åº¦èª¿ç ” (Deep Research)**ï¼Œè€Œä¸åƒ…åƒ…æ˜¯æŸ¥å®šç¾©ã€‚
   - **åŸ·è¡Œè¦å‰‡ (Critical)**ï¼š
     - **è‹¥ç„¡ä¸Šå‚³æ–‡ä»¶**ï¼š**å¿…é ˆ**å‘¼å« `Google Search`ã€‚
       - **ã€æœå°‹æŠ€å·§å¼·åŒ–ã€‘**ï¼šä¸è¦åªæœ "é‡å­åŠ›å­¸"ã€‚ä½ å¿…é ˆä¸»å‹•æ‹†è§£æ„åœ–ï¼Œæœå°‹å…·é«”çš„é«˜åƒ¹å€¼è³‡è¨Šã€‚
       - **æœå°‹è©ç¯„ä¾‹**ï¼š
         - "é‡å­åŠ›å­¸ æœ€æ–°æ‡‰ç”¨è¶¨å‹¢" (æ‰¾è¶¨å‹¢)
         - "é‡å­åŠ›å­¸ å¸‚å ´è¦æ¨¡ é æ¸¬" (æ‰¾æ•¸æ“š)
         - "é‡å­åŠ›å­¸ ä¸»è¦æŒ‘æˆ°èˆ‡ç“¶é ¸" (æ‰¾ç—›é»)
       - *è«‹å˜—è©¦åœ¨ä¸€æ¬¡å›æ‡‰ä¸­ï¼Œæˆ–é€éé€£çºŒçš„å·¥å…·å‘¼å«ï¼Œè’é›†å¤šé¢å‘è³‡è¨Šã€‚*
     - **è‹¥æœ‰ä¸Šå‚³æ–‡ä»¶**ï¼šå„ªå…ˆå‘¼å« `read_knowledge_base`ï¼ŒæŸ¥è©¢æ–‡ä»¶å…§é—œæ–¼è©²ä¸»é¡Œçš„**æ•¸æ“šèˆ‡çµè«–**ã€‚

2. **ç‹€æ³ Bï¼šä½¿ç”¨è€…æ˜ç¢ºè¦æ±‚ã€Œæœå°‹ã€**
   - ä¾ç…§ä½¿ç”¨è€…çš„æŒ‡ç¤ºï¼Œä½†åŒæ¨£å¥—ç”¨ä¸Šè¿°çš„ã€Œæœå°‹æŠ€å·§å¼·åŒ–ã€ï¼Œä¸»å‹•å„ªåŒ–ä½¿ç”¨è€…çš„é—œéµå­—ã€‚

3. **ç‹€æ³ Cï¼šæ¨¡ç³ŠæŒ‡ä»¤**
   - å¦‚æœå®Œå…¨ç„¡æ³•åˆ¤æ–·ï¼Œæ‰åå•ã€‚
   - è‹¥åªçµ¦åè©ï¼Œ**é è¨­é‚£å°±æ˜¯ä¸»é¡Œ**ï¼Œç›´æ¥é–‹å·¥ã€‚

### åƒæ•¸å¡«å¯«éµå¾‹ï¼š
- **åš´ç¦å‘¼å«ç©ºåƒæ•¸**ã€‚
- å¦‚æœä½ è¦å‘¼å« `Google Search`ï¼Œ`query` è«‹ç›´æ¥å¡«å…¥ä½ å„ªåŒ–éçš„é—œéµå­—ã€‚
"""

# Init
st.set_page_config(page_title="Smart Deck Agent", page_icon="ğŸ“Š", layout="wide")
try: Config.validate()
except Exception as e: st.error(f"ç’°å¢ƒè¨­å®šéŒ¯èª¤: {e}"); st.stop()

if "app_initialized" not in st.session_state:
    reset_vector_store()
    st.session_state.app_initialized = True

# LLM
tools = [rag_tool, search_tool]
tool_map = {"read_knowledge_base": rag_tool, "google_search": search_tool}
llm = ChatGoogleGenerativeAI(
    model=Config.MODEL_FAST, google_api_key=Config.GOOGLE_API_KEY, temperature=0.7
)
llm_with_tools = llm.bind_tools(tools)

# Session
if "messages" not in st.session_state: st.session_state.messages = []
if "db_files" not in st.session_state: st.session_state.db_files = set() 
if "file_uploader_key" not in st.session_state: st.session_state.file_uploader_key = 0

# Sidebar
with st.sidebar:
    st.header("ğŸ“‚ è³‡æ–™ä¾†æº")
    uploaded_files = st.file_uploader("ä¸Šå‚³ PDF/TXT", type=["pdf", "txt"], accept_multiple_files=True, key=f"uploader_{st.session_state.file_uploader_key}")
    
    if uploaded_files:
        current_filenames = {f.name for f in uploaded_files}
        new_files = [f for f in uploaded_files if f.name not in st.session_state.db_files]
        for file in new_files:
            with st.spinner(f"è™•ç†ä¸­ï¼š{file.name}..."):
                temp_path = os.path.join(Config.UPLOAD_DIR, file.name)
                with open(temp_path, "wb") as f: f.write(file.getbuffer())
                res = ingest_file(temp_path)
                if "æˆåŠŸ" in res:
                    st.session_state.db_files.add(file.name)
                    st.session_state.messages.append(HumanMessage(content=f"[ç³»çµ±] å·²ä¸Šå‚³ {file.name}"))
                else: st.error(res)

    st.divider()
    if st.button("ğŸ—‘ï¸ Reset", type="secondary"):
        reset_vector_store()
        st.session_state.db_files = set()
        st.session_state.messages = []
        st.session_state.file_uploader_key += 1
        st.rerun()

    if st.button("âœ¨ ç”Ÿæˆ PPT", type="primary"):
        with st.status("ğŸ¤– AI åœ˜éšŠå·¥ä½œä¸­...", expanded=True) as status:
            chat_history = "\n".join([f"{type(m).__name__}: {m.content}" for m in st.session_state.messages])
            
            status.write("ğŸ§  Manager: æ­£åœ¨è¦åŠƒç°¡å ±æ¶æ§‹...")
            final_state = agent_workflow.invoke({"user_request": "è£½ä½œç°¡å ±", "chat_history": chat_history})
            
            status.write("âœï¸ Writer: æ­£åœ¨æ’ç‰ˆèˆ‡è£½ä½œæŠ•å½±ç‰‡...")
            if final_state.get("final_file_path"):
                with open(final_state["final_file_path"], "rb") as f:
                    st.download_button("ğŸ“¥ ä¸‹è¼‰ PPT", f, os.path.basename(final_state["final_file_path"]))
                status.update(label="âœ… å®Œæˆï¼", state="complete")
            else: status.error("ç”Ÿæˆå¤±æ•—ï¼Œè«‹æª¢æŸ¥ Logã€‚")

# Chat Interface
st.title("ğŸ’¬ Smart Deck Agent")

for msg in st.session_state.messages:
    if isinstance(msg, HumanMessage):
        with st.chat_message("user"): st.markdown(msg.content)
    elif isinstance(msg, AIMessage) and msg.content:
        with st.chat_message("assistant"): st.markdown(msg.content)

if prompt := st.chat_input("è¼¸å…¥è¨Šæ¯ (ä¾‹å¦‚ï¼šæƒ³äº†è§£çš„ä¸»é¡Œã€ä¸Šå‚³æ–‡ä»¶å¾Œåˆ†æ)..."):
    st.session_state.messages.append(HumanMessage(content=prompt))
    with st.chat_message("user"): st.markdown(prompt)

    with st.chat_message("assistant"):
        status_box = st.empty()
        
        with st.spinner("æ€è€ƒä¸­..."):
            try:
                # 1. æ³¨å…¥å‹•æ…‹ Prompt (å‘ŠçŸ¥æª”æ¡ˆç‹€æ…‹)
                file_count = len(st.session_state.db_files)
                file_names = ", ".join(st.session_state.db_files) if file_count > 0 else "ç„¡"
                
                dynamic_system_prompt = SYSTEM_PROMPT_TEMPLATE.format(
                    file_count=file_count,
                    file_names=file_names
                )
                
                messages_to_send = [SystemMessage(content=dynamic_system_prompt)] + st.session_state.messages
                
                # 2. ç¬¬ä¸€æ¬¡å‘¼å« LLM
                response = llm_with_tools.invoke(messages_to_send)
                st.session_state.messages.append(response)

                # 3. å·¥å…·è¿´åœˆ (è’é›†è³‡è¨Š)
                while response.tool_calls:
                    for tool_call in response.tool_calls:
                        name = tool_call["name"]
                        args = tool_call["args"]
                        query_val = args.get("query", "")
                        
                        # [é˜²å‘†] è‹¥ LLM çµ¦ç©ºåƒæ•¸ï¼Œç›´æ¥ç”¨ä½¿ç”¨è€…çš„ Prompt (å³ä¸»é¡Œ)
                        if not query_val:
                            query_val = prompt
                            tool_call["args"]["query"] = prompt 
                        
                        # [UI] é¡¯ç¤ºè¦ªåˆ‡åç¨± + æŸ¥è©¢å…§å®¹
                        display_name = TOOL_DISPLAY_NAMES.get(name, f"ğŸ”§ {name}")
                        status_box.info(f"{display_name}ï¼š{query_val}")
                        
                        # åŸ·è¡Œå·¥å…·
                        tool = tool_map.get(name)
                        output = tool.invoke(query_val) if tool else "Error: Tool not found"
                        
                        st.session_state.messages.append(
                            ToolMessage(content=str(output), tool_call_id=tool_call["id"], name=name)
                        )
                    
                    # å†æ¬¡å‘¼å« LLM (å¸¶è‘—æœå°‹çµæœ)
                    messages_to_send = [SystemMessage(content=dynamic_system_prompt)] + st.session_state.messages
                    response = llm_with_tools.invoke(messages_to_send)
                    st.session_state.messages.append(response)
                
                status_box.empty()
                st.markdown(response.content)

            except Exception as e:
                st.error(f"Error: {e}")