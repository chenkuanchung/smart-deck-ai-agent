# src/app.py
import streamlit as st
import os
import sys

# è·¯å¾‘ä¿®æ­£
current_file_path = os.path.abspath(__file__)
src_dir = os.path.dirname(current_file_path)
project_root = os.path.dirname(src_dir)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# Import
from src.config import Config
from src.tools.rag import ingest_file, rag_tool, reset_vector_store, remove_file_from_db
from src.tools.search import search_tool
from src.graph import agent_workflow
from src.agents.state import AgentState
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, SystemMessage

# --- System Prompt (å‰ç«¯ Chat Agent å°ˆç”¨) ---
# [é—œéµå„ªåŒ–] åŠ å…¥ã€Œå°è©±ç¯„ä¾‹ (Few-Shot)ã€æ•™å° Agent æ­£ç¢ºçš„å·¥å…·ä½¿ç”¨æ™‚æ©Ÿ
SYSTEM_PROMPT = """
ä½ ä¸å†åªæ˜¯ä¸€å€‹ç°¡å–®çš„å•ç­”æ©Ÿå™¨äººï¼Œä½ æ˜¯ **Smart Deck åœ˜éšŠçš„ã€Œé¦–å¸­ç ”ç©¶åˆ†æå¸« (Lead Research Analyst)ã€**ã€‚

### ä½ çš„æ ¸å¿ƒç›®æ¨™ï¼š
ä½ çš„å·¥ä½œæ˜¯ç‚ºå¾Œç«¯çš„ Manager æä¾›**ã€Œé«˜è³‡è¨Šå¯†åº¦ (High Information Density)ã€**çš„ç°¡å ±ç´ æã€‚
ä½¿ç”¨è€…å‘Šè¨´ä½ ä¸»é¡Œï¼ˆä¾‹å¦‚ã€Œç”Ÿæˆå¼ AIã€ï¼‰ï¼Œä½ ä¸èƒ½åªçµ¦å®šç¾©ã€‚ä½ å¿…é ˆæŒ–æ˜å‡ºèƒ½æ”¯æ’ä¸€ä»½ 10 é å°ˆæ¥­ç°¡å ±çš„æ·±åº¦å…§å®¹ã€‚

### ä½ çš„æ€è€ƒèˆ‡è¡Œå‹•æº–å‰‡ (Research Protocol)ï¼š
1. **æ‹’çµ•æ·ºå±¤è³‡è¨Š**ï¼šä¸è¦åªçµ¦ã€Œæ˜¯ä»€éº¼ã€ï¼Œè¦çµ¦ã€Œç‚ºä»€éº¼ã€ã€ã€Œå¤šå°‘éŒ¢ã€ã€ã€Œæˆé•·ç‡å¤šå°‘ã€ã€ã€Œèª°åœ¨åšã€ã€‚
2. **æ•¸æ“šå„ªå…ˆ (Data First)**ï¼šç°¡å ±éœ€è¦èªªæœåŠ›ã€‚æœå°‹æ™‚å„ªå…ˆå°‹æ‰¾ï¼š
   - **å…·é«”æ•¸å­—** (Market Size, CAGR, Revenue)
   - **æ™‚é–“æˆ³è¨˜** (2024 Q3 æœ€æ–°å ±å‘Š, 2025 é æ¸¬)
   - **å…·é«”æ¡ˆä¾‹** (Company Use Cases, Competitor Analysis)
3. **å¤šè§’åº¦æ‹†è§£ (Multi-hop Reasoning)**ï¼š
   - ç•¶ä½¿ç”¨è€…èªªã€Œæˆ‘è¦åšé—œæ–¼ X çš„ç°¡å ±ã€æ™‚ï¼Œä¸è¦åªæœå°‹ "X"ã€‚
   - **ä½ å¿…é ˆä¸»å‹•æ‹†è§£æœå°‹**ï¼ˆå³ä½¿ä½ éœ€è¦å¤šå‘¼å«å¹¾æ¬¡ google_searchï¼‰ï¼š
     - æœå°‹ 1: "X 2024 å¸‚å ´è¦æ¨¡èˆ‡æˆé•·ç‡"
     - æœå°‹ 2: "X çš„ä¸»è¦æ‡‰ç”¨å ´æ™¯èˆ‡æ¡ˆä¾‹"
     - æœå°‹ 3: "X çš„æŠ€è¡“æŒ‘æˆ°èˆ‡ç¼ºé»"
     - æœå°‹ 4: "X çš„é ˜å°å» å•†èˆ‡ç«¶å“æ¯”è¼ƒ"

### å·¥å…·ä½¿ç”¨ç­–ç•¥ (Tool Use Policy)ï¼š
- **read_knowledge_base**: ç•¶ä½¿ç”¨è€…å•åŠå…§éƒ¨æ–‡ä»¶ã€ä¸Šå‚³çš„ PDF ç´°ç¯€æ™‚ä½¿ç”¨ã€‚
- **google_search**: 
   - **ä¸è¦åªæœåè©**ã€‚ä¾‹å¦‚ä¸è¦æœ "AI"ï¼Œè¦æœ "AI 2024 trends statistical report"ã€‚
   - å¦‚æœç¬¬ä¸€æ¬¡æœå°‹çµæœå¤ªéç©ºæ³›ï¼Œ**è«‹ä¸»å‹•æ›å€‹é—œéµå­—å†æœä¸€æ¬¡**ï¼Œç›´åˆ°ä½ æ”¶é›†åˆ°è¶³å¤ çš„æ•¸æ“šã€‚

### æ‡‰å°æ¨¡ç³ŠæŒ‡ä»¤ï¼š
- è‹¥ä½¿ç”¨è€…åªèªªã€Œå¹«æˆ‘æŸ¥ã€ï¼Œè«‹åå•ï¼šã€Œæˆ‘å€‘è¦èšç„¦åœ¨å“ªå€‹é¢å‘ï¼Ÿä¾‹å¦‚æŠ€è¡“æ¶æ§‹ã€å¸‚å ´åˆ†æï¼Œé‚„æ˜¯ç«¶çˆ­å°æ‰‹ï¼Ÿã€
- ä½†è‹¥ä½¿ç”¨è€…èªªã€Œæˆ‘è¦åšé—œæ–¼ [ä¸»é¡Œ] çš„ç°¡å ±ã€ï¼Œ**è«‹ç›´æ¥å•Ÿå‹•å…¨æ–¹ä½æœå°‹**ï¼Œä¸éœ€è¦å†å•ä½¿ç”¨è€…ï¼Œå±•ç¾ä½ çš„ä¸»å‹•æ€§ã€‚

### å°è©±ç¯„ä¾‹ (Few-Shot Examples)ï¼š

User: "æˆ‘æƒ³åšä¸€ä»½é—œæ–¼é›»å‹•è»Šé›»æ± çš„ç°¡å ±"
AI Thought: ä½¿ç”¨è€…è¦ç°¡å ±ï¼Œæˆ‘ä¸èƒ½åªçµ¦å®šç¾©ã€‚æˆ‘éœ€è¦å¸‚å ´æ•¸æ“šã€æŠ€è¡“åˆ†é¡(å›ºæ…‹vsé‹°é›¢å­)ã€ä¸»è¦å» å•†ã€‚
AI Action: 
   1. google_search("EV battery market size 2024 2030 CAGR")
   2. google_search("Solid-state battery vs Lithium-ion pros cons")
   3. google_search("Top EV battery manufacturers market share 2024")
AI Response: (å½™æ•´æ‰€æœ‰æ•¸æ“šï¼Œæä¾›çµæ§‹åŒ–çš„å›ç­”ï¼ŒåŒ…å«æ•¸æ“šä¾†æºèˆ‡å¹´ä»½)

User: "æœå°‹é€™ä»½æ–‡ä»¶çš„é‡é»"
AI: (å‘¼å«å·¥å…·) read_knowledge_base(query="æ–‡ä»¶ æ ¸å¿ƒçµè«– èˆ‡ é—œéµæ•¸æ“š")
"""

# 1. Init
st.set_page_config(page_title="Smart Deck Agent", page_icon="ğŸ“Š", layout="wide")
try:
    Config.validate()
except Exception as e:
    st.error(f"ç’°å¢ƒè¨­å®šéŒ¯èª¤: {e}")
    st.stop()

# --- åˆå§‹åŒ–æª¢æŸ¥é‚è¼¯ (Reset on Refresh) ---
if "app_initialized" not in st.session_state:
    print("ğŸ”„ åµæ¸¬åˆ°æ–° Session æˆ–é é¢åˆ·æ–°ï¼Œæ­£åœ¨åŸ·è¡Œç’°å¢ƒé‡ç½®...")
    reset_vector_store()
    st.session_state.app_initialized = True

# 2. LLM (Chat Agent - æ“æœ‰æ‰€æœ‰å·¥å…·)
# å‰ç«¯ Chat Agent é‚„æ˜¯éœ€è¦ RAGï¼Œé€™æ¨£ä½¿ç”¨è€…å•ã€ŒPDFè£¡å¯«ä»€éº¼ï¼Ÿã€å®ƒæ‰ç­”å¾—å‡ºä¾†
tools = [rag_tool, search_tool]
tool_map = {"read_knowledge_base": rag_tool, "google_search": search_tool}

llm = ChatGoogleGenerativeAI(
    model=Config.MODEL_FAST,
    google_api_key=Config.GOOGLE_API_KEY,
    temperature=0.3
)
llm_with_tools = llm.bind_tools(tools)

# 3. Session State
if "messages" not in st.session_state:
    st.session_state.messages = [SystemMessage(content=SYSTEM_PROMPT)]

if "db_files" not in st.session_state:
    st.session_state.db_files = set() 

if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = 0

# 4. å´é‚Šæ¬„
with st.sidebar:
    st.header("ğŸ“‚ è³‡æ–™ä¾†æº")
    
    uploaded_files = st.file_uploader(
        "é¸æ“‡æª”æ¡ˆ (PDF/TXT)", 
        type=["pdf", "txt"], 
        accept_multiple_files=True,
        key=f"uploader_{st.session_state.file_uploader_key}"
    )
    
    if uploaded_files is not None:
        current_ui_filenames = {f.name for f in uploaded_files}
        new_files = [f for f in uploaded_files if f.name not in st.session_state.db_files]
        removed_files = st.session_state.db_files - current_ui_filenames
        
        for file in new_files:
            with st.spinner(f"æ­£åœ¨è™•ç†æ–°æª”æ¡ˆï¼š{file.name}..."):
                temp_path = os.path.join(Config.UPLOAD_DIR, file.name)
                with open(temp_path, "wb") as f:
                    f.write(file.getbuffer())
                
                result = ingest_file(temp_path)
                if "æˆåŠŸ" in result:
                    st.success(result)
                    st.session_state.db_files.add(file.name)
                    st.session_state.messages.append(
                        HumanMessage(content=f"[ç³»çµ±é€šçŸ¥] æˆ‘ä¸Šå‚³äº† '{file.name}'ã€‚")
                    )
                else:
                    st.error(result)

        for filename in removed_files:
            with st.spinner(f"æ­£åœ¨ç§»é™¤æª”æ¡ˆï¼š{filename}..."):
                msg = remove_file_from_db(filename)
                st.warning(msg)
                st.session_state.db_files.remove(filename)
                st.session_state.messages.append(
                    HumanMessage(content=f"[ç³»çµ±é€šçŸ¥] æˆ‘ç§»é™¤äº† '{filename}'ï¼Œè«‹ä¸è¦å†åƒè€ƒå®ƒçš„å…§å®¹ã€‚")
                )
    
    st.divider()
    
    if st.button("ğŸ—‘ï¸ Reset å…¨éƒ¨", type="secondary"):
        reset_vector_store()
        st.session_state.db_files = set()
        st.session_state.messages = [SystemMessage(content=SYSTEM_PROMPT)]
        st.session_state.file_uploader_key += 1
        st.rerun()

    st.header("ğŸš€ ç”Ÿæˆè¡Œå‹•")
    if st.button("âœ¨ ç”Ÿæˆ PPT ç°¡å ±", type="primary"):
        with st.status("ğŸ¤– AI åœ˜éšŠå·¥ä½œä¸­...", expanded=True) as status:
            chat_history = ""
            # æ”¶é›† Chat Agent è¾›è‹¦æœå°‹ä¾†çš„è³‡è¨Š
            for msg in st.session_state.messages:
                if isinstance(msg, HumanMessage):
                    chat_history += f"User: {msg.content}\n"
                elif isinstance(msg, AIMessage) and msg.content:
                    chat_history += f"AI: {msg.content}\n"
            
            status.write("ğŸ§  Manager æ­£åœ¨åˆ†ææ–‡ä»¶ä¸¦è¦åŠƒæ¶æ§‹...")
            
            # ç›´æ¥å‘¼å« Graphï¼Œè®“ Manager è‡ªå·±å»æ±ºå®šè¦ä¸è¦è®€æª”
            initial_state = {"user_request": "è«‹è£½ä½œä¸€ä»½ç°¡å ±", "chat_history": chat_history}
            final_state = agent_workflow.invoke(initial_state)
            
            status.write("âœï¸ Writer æ’°å¯«èˆ‡æ’ç‰ˆ...")
            if final_state.get("final_file_path"):
                ppt_path = final_state["final_file_path"]
                file_name = os.path.basename(ppt_path)
                with open(ppt_path, "rb") as f:
                    st.download_button("ğŸ“¥ ä¸‹è¼‰ PPT", f, file_name)
                status.update(label="âœ… å®Œæˆï¼", state="complete")
            else:
                status.error("ç”Ÿæˆå¤±æ•—")

# 5. ä¸»èŠå¤©å€
st.title("ğŸ’¬ Smart Deck Agent")

for msg in st.session_state.messages:
    if isinstance(msg, HumanMessage):
        with st.chat_message("user"):
            st.markdown(msg.content)
    elif isinstance(msg, AIMessage) and msg.content:
        with st.chat_message("assistant"):
            st.markdown(msg.content)

if prompt := st.chat_input("è¼¸å…¥è¨Šæ¯..."):
    st.session_state.messages.append(HumanMessage(content=prompt))
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        status_box = st.empty()
        with st.spinner("æ€è€ƒä¸­..."):
            try:
                response = llm_with_tools.invoke(st.session_state.messages)
                st.session_state.messages.append(response)

                if response.tool_calls:
                    for tool_call in response.tool_calls:
                        name = tool_call["name"]
                        args = tool_call["args"]
                        call_id = tool_call["id"]
                        
                        search_term = args.get("query")
                        if name == "read_knowledge_base":
                            status_box.info(f"ğŸ“š æŸ¥é–±è³‡æ–™åº«: {search_term}")
                        elif name == "google_search":
                            status_box.info(f"ğŸŒ æœå°‹ç¶²è·¯: {search_term}")
                        
                        tool = tool_map.get(name)
                        output = "Error: Tool not found"
                        if tool:
                            query_val = search_term if search_term else "ç¸½çµ" 
                            try:
                                output = tool.invoke(query_val)
                            except Exception as e:
                                output = f"Error: {e}"
                        
                        st.session_state.messages.append(
                            ToolMessage(content=str(output), tool_call_id=call_id, name=name)
                        )
                    
                    final_res = llm_with_tools.invoke(st.session_state.messages)
                    st.markdown(final_res.content)
                    st.session_state.messages.append(final_res)
                    status_box.empty()
                else:
                    st.markdown(response.content)
            except Exception as e:

                st.error(f"Error: {e}")
