# src/app.py
import streamlit as st
import os
import sys

# è·¯å¾‘ä¿®æ­£
current_file_path = os.path.abspath(__file__)
src_dir = os.path.dirname(current_file_path)
project_root = os.path.dirname(src_dir)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# Import
from src.config import Config
from src.tools.rag import ingest_file, rag_tool, reset_vector_store, remove_file_from_db
from src.tools.search import search_tool
from src.graph import agent_workflow
from src.agents.state import AgentState
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, SystemMessage

# --- System Prompt (å‰ç«¯ Chat Agent å°ˆç”¨) ---
# [é—œéµå„ªåŒ–] åŠ å…¥ã€Œå°è©±ç¯„ä¾‹ (Few-Shot)ã€æ•™å° Agent æ­£ç¢ºçš„å·¥å…·ä½¿ç”¨æ™‚æ©Ÿ
SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€å€‹æ™ºæ…§å‹æ–‡ä»¶åˆ†æèˆ‡ç°¡å ±åŠ©æ‰‹ (Smart Deck Agent)ã€‚

### ä½ çš„è§’è‰²åˆ†å·¥ï¼š
1. **ä½ æ˜¯ã€Œå‰ç«¯è³‡è¨Šè’é›†å“¡ã€**ï¼šè² è²¬å›ç­”ä½¿ç”¨è€…çš„å•é¡Œï¼Œä¸¦è’é›†å¿…è¦çš„èƒŒæ™¯è³‡è¨Šã€‚
2. **ç°¡å ±è£½ä½œç”±å¾Œç«¯ Manager è² è²¬**ï¼šä½ ä¸éœ€è¦è‡ªå·±ç”¢ç”Ÿ PPT ä»£ç¢¼ï¼Œåªéœ€ç¢ºèªä½¿ç”¨è€…çš„éœ€æ±‚ã€‚

### å·¥å…·ä½¿ç”¨ç­–ç•¥ (Tool Use Policy)ï¼š
1. **æ–‡ä»¶å•é¡Œ**ï¼šä½¿ç”¨ `read_knowledge_base`ã€‚
2. **å¤–éƒ¨è³‡è¨Š**ï¼šä½¿ç”¨ `Google Search` æŸ¥è©¢æœ€æ–°æ–°èã€æ•¸æ“šæˆ–ç«¶å“è³‡è¨Šã€‚
3. **æ¨¡ç³ŠæŒ‡ä»¤è™•ç† (é‡è¦)**ï¼š
   - å¦‚æœä½¿ç”¨è€…åªèªªã€Œä¸Šç¶²æœå°‹ã€ã€ã€Œå¹«æˆ‘æŸ¥ã€ä½†**æ²’èªªè¦æŸ¥ä»€éº¼**ï¼Œ**è«‹ä¸è¦å‘¼å«å·¥å…·**ã€‚
   - è«‹ç›´æ¥å›ç­”ï¼šã€Œè«‹å•æ‚¨æƒ³æœå°‹ä»€éº¼å…§å®¹ï¼Ÿè«‹æä¾›å…·é«”çš„é—œéµå­—ã€‚ã€

### å°è©±ç¯„ä¾‹ (Examples)ï¼š
User: "ä¸Šç¶²æœå°‹"
AI: (ä¸å‘¼å«å·¥å…·) "è«‹å•æ‚¨æƒ³æœå°‹ä»€éº¼ä¸»é¡Œï¼Ÿä¾‹å¦‚ï¼š'æœ€æ–°çš„ AI è¶¨å‹¢'ã€‚"

User: "æœå°‹é‡å­åŠ›å­¸çš„å®šç¾©"
AI: (å‘¼å«å·¥å…·) google_search(query="é‡å­åŠ›å­¸ å®šç¾©")

User: "é€™ä»½æ–‡ä»¶åœ¨è¬›ä»€éº¼ï¼Ÿ"
AI: (å‘¼å«å·¥å…·) read_knowledge_base(query="æ–‡ä»¶ é‡é»æ‘˜è¦")
"""

# 1. Init
st.set_page_config(page_title="Smart Deck Agent", page_icon="ğŸ“Š", layout="wide")
try:
    Config.validate()
except Exception as e:
    st.error(f"ç’°å¢ƒè¨­å®šéŒ¯èª¤: {e}")
    st.stop()

# --- åˆå§‹åŒ–æª¢æŸ¥é‚è¼¯ (Reset on Refresh) ---
if "app_initialized" not in st.session_state:
    print("ğŸ”„ åµæ¸¬åˆ°æ–° Session æˆ–é é¢åˆ·æ–°ï¼Œæ­£åœ¨åŸ·è¡Œç’°å¢ƒé‡ç½®...")
    reset_vector_store()
    st.session_state.app_initialized = True

# 2. LLM (Chat Agent - æ“æœ‰æ‰€æœ‰å·¥å…·)
# å‰ç«¯ Chat Agent é‚„æ˜¯éœ€è¦ RAGï¼Œé€™æ¨£ä½¿ç”¨è€…å•ã€ŒPDFè£¡å¯«ä»€éº¼ï¼Ÿã€å®ƒæ‰ç­”å¾—å‡ºä¾†
tools = [rag_tool, search_tool]
tool_map = {"read_knowledge_base": rag_tool, "google_search": search_tool}

llm = ChatGoogleGenerativeAI(
    model=Config.MODEL_FAST,
    google_api_key=Config.GOOGLE_API_KEY,
    temperature=0.3
)
llm_with_tools = llm.bind_tools(tools)

# 3. Session State
if "messages" not in st.session_state:
    st.session_state.messages = [SystemMessage(content=SYSTEM_PROMPT)]

if "db_files" not in st.session_state:
    st.session_state.db_files = set() 

if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = 0

# 4. å´é‚Šæ¬„
with st.sidebar:
    st.header("ğŸ“‚ è³‡æ–™ä¾†æº")
    
    uploaded_files = st.file_uploader(
        "é¸æ“‡æª”æ¡ˆ (PDF/TXT)", 
        type=["pdf", "txt"], 
        accept_multiple_files=True,
        key=f"uploader_{st.session_state.file_uploader_key}"
    )
    
    if uploaded_files is not None:
        current_ui_filenames = {f.name for f in uploaded_files}
        new_files = [f for f in uploaded_files if f.name not in st.session_state.db_files]
        removed_files = st.session_state.db_files - current_ui_filenames
        
        for file in new_files:
            with st.spinner(f"æ­£åœ¨è™•ç†æ–°æª”æ¡ˆï¼š{file.name}..."):
                temp_path = os.path.join(Config.UPLOAD_DIR, file.name)
                with open(temp_path, "wb") as f:
                    f.write(file.getbuffer())
                
                result = ingest_file(temp_path)
                if "æˆåŠŸ" in result:
                    st.success(result)
                    st.session_state.db_files.add(file.name)
                    st.session_state.messages.append(
                        HumanMessage(content=f"[ç³»çµ±é€šçŸ¥] æˆ‘ä¸Šå‚³äº† '{file.name}'ã€‚")
                    )
                else:
                    st.error(result)

        for filename in removed_files:
            with st.spinner(f"æ­£åœ¨ç§»é™¤æª”æ¡ˆï¼š{filename}..."):
                msg = remove_file_from_db(filename)
                st.warning(msg)
                st.session_state.db_files.remove(filename)
                st.session_state.messages.append(
                    HumanMessage(content=f"[ç³»çµ±é€šçŸ¥] æˆ‘ç§»é™¤äº† '{filename}'ï¼Œè«‹ä¸è¦å†åƒè€ƒå®ƒçš„å…§å®¹ã€‚")
                )
    
    st.divider()
    
    if st.button("ğŸ—‘ï¸ Reset å…¨éƒ¨", type="secondary"):
        reset_vector_store()
        st.session_state.db_files = set()
        st.session_state.messages = [SystemMessage(content=SYSTEM_PROMPT)]
        st.session_state.file_uploader_key += 1
        st.rerun()

    st.header("ğŸš€ ç”Ÿæˆè¡Œå‹•")
    if st.button("âœ¨ ç”Ÿæˆ PPT ç°¡å ±", type="primary"):
        with st.status("ğŸ¤– AI åœ˜éšŠå·¥ä½œä¸­...", expanded=True) as status:
            chat_history = ""
            # æ”¶é›† Chat Agent è¾›è‹¦æœå°‹ä¾†çš„è³‡è¨Š
            for msg in st.session_state.messages:
                if isinstance(msg, HumanMessage):
                    chat_history += f"User: {msg.content}\n"
                elif isinstance(msg, AIMessage) and msg.content:
                    chat_history += f"AI: {msg.content}\n"
            
            status.write("ğŸ§  Manager æ­£åœ¨åˆ†ææ–‡ä»¶ä¸¦è¦åŠƒæ¶æ§‹...")
            
            # ç›´æ¥å‘¼å« Graphï¼Œè®“ Manager è‡ªå·±å»æ±ºå®šè¦ä¸è¦è®€æª”
            initial_state = {"user_request": "è«‹è£½ä½œä¸€ä»½ç°¡å ±", "chat_history": chat_history}
            final_state = agent_workflow.invoke(initial_state)
            
            status.write("âœï¸ Writer æ’°å¯«èˆ‡æ’ç‰ˆ...")
            if final_state.get("final_file_path"):
                ppt_path = final_state["final_file_path"]
                file_name = os.path.basename(ppt_path)
                with open(ppt_path, "rb") as f:
                    st.download_button("ğŸ“¥ ä¸‹è¼‰ PPT", f, file_name)
                status.update(label="âœ… å®Œæˆï¼", state="complete")
            else:
                status.error("ç”Ÿæˆå¤±æ•—")

# 5. ä¸»èŠå¤©å€
st.title("ğŸ’¬ Smart Deck Agent")

for msg in st.session_state.messages:
    if isinstance(msg, HumanMessage):
        with st.chat_message("user"):
            st.markdown(msg.content)
    elif isinstance(msg, AIMessage) and msg.content:
        with st.chat_message("assistant"):
            st.markdown(msg.content)

if prompt := st.chat_input("è¼¸å…¥è¨Šæ¯..."):
    st.session_state.messages.append(HumanMessage(content=prompt))
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        status_box = st.empty()
        with st.spinner("æ€è€ƒä¸­..."):
            try:
                response = llm_with_tools.invoke(st.session_state.messages)
                st.session_state.messages.append(response)

                if response.tool_calls:
                    for tool_call in response.tool_calls:
                        name = tool_call["name"]
                        args = tool_call["args"]
                        call_id = tool_call["id"]
                        
                        search_term = args.get("query")
                        if name == "read_knowledge_base":
                            status_box.info(f"ğŸ“š æŸ¥é–±è³‡æ–™åº«: {search_term}")
                        elif name == "google_search":
                            status_box.info(f"ğŸŒ æœå°‹ç¶²è·¯: {search_term}")
                        
                        tool = tool_map.get(name)
                        output = "Error: Tool not found"
                        if tool:
                            query_val = search_term if search_term else "ç¸½çµ" 
                            try:
                                output = tool.invoke(query_val)
                            except Exception as e:
                                output = f"Error: {e}"
                        
                        st.session_state.messages.append(
                            ToolMessage(content=str(output), tool_call_id=call_id, name=name)
                        )
                    
                    final_res = llm_with_tools.invoke(st.session_state.messages)
                    st.markdown(final_res.content)
                    st.session_state.messages.append(final_res)
                    status_box.empty()
                else:
                    st.markdown(response.content)
            except Exception as e:
                st.error(f"Error: {e}")