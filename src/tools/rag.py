# src/tools/rag.py
import os
import shutil
import glob
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_core.tools import Tool
from src.config import Config  # è¨˜å¾— import Config

# è¨­å®šè·¯å¾‘
PERSIST_DIRECTORY = os.path.join(os.getcwd(), "chroma_db")

# åˆå§‹åŒ– Embedding
embeddings = GoogleGenerativeAIEmbeddings(
    model=Config.MODEL_EMBEDDING,
    google_api_key=Config.GOOGLE_API_KEY
)

vector_store = Chroma(
    collection_name="smart_deck_docs",
    embedding_function=embeddings,
    persist_directory=PERSIST_DIRECTORY
)

def ingest_file(file_path: str):
    """è®€å–æª”æ¡ˆä¸¦å­˜å…¥å‘é‡åº«"""
    # ç¢ºä¿è·¯å¾‘æ˜¯çµ•å°è·¯å¾‘
    file_path = os.path.abspath(file_path)
    
    if not os.path.exists(file_path):
        return f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æª”æ¡ˆ {file_path}"
    
    try:
        # è¼‰å…¥å™¨é‚è¼¯
        if file_path.lower().endswith('.pdf'):
            loader = PyPDFLoader(file_path)
        elif file_path.lower().endswith('.txt'):
            loader = TextLoader(file_path, encoding='utf-8')
        else:
            return "âŒ ç›®å‰åªæ”¯æ´ PDF èˆ‡ TXT"

        docs = loader.load()
        # å°‡ source æ¨™è¨˜ç‚ºçµ•å°è·¯å¾‘
        for doc in docs:
            doc.metadata["source"] = file_path
            
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)
        
        if splits:
            vector_store.add_documents(documents=splits)
            return f"âœ… æˆåŠŸï¼šå·²å°‡ {os.path.basename(file_path)} å­˜å…¥çŸ¥è­˜åº«ã€‚"
        else:
            return "âš ï¸ æª”æ¡ˆå…§å®¹ç‚ºç©ºã€‚"
            
    except Exception as e:
        return f"âŒ è®€å–å¤±æ•—ï¼š{str(e)}"

def remove_file_from_db(filename: str):
    """å¾å‘é‡è³‡æ–™åº«ä¸­ç§»é™¤æŒ‡å®šæª”æ¡ˆï¼Œä¸¦åˆªé™¤å¯¦é«”æª”æ¡ˆ"""
    try:
        # [ä¿®æ”¹] è·¯å¾‘æŒ‡å‘ Config.UPLOAD_DIR
        file_path = os.path.abspath(os.path.join(Config.UPLOAD_DIR, filename))
        msg = []

        # 1. è™•ç†è³‡æ–™åº«åˆªé™¤
        existing_docs = vector_store.get(where={"source": file_path})
        if existing_docs['ids']:
            vector_store.delete(ids=existing_docs['ids'])
            msg.append(f"è³‡æ–™åº«ç´€éŒ„å·²ç§»é™¤ ({len(existing_docs['ids'])} ç­†)")
        else:
            msg.append("è³‡æ–™åº«ä¸­ç„¡æ­¤ç´€éŒ„")

        # 2. è™•ç†å¯¦é«”æª”æ¡ˆåˆªé™¤
        if os.path.exists(file_path):
            os.remove(file_path)
            msg.append("å¯¦é«”æª”æ¡ˆå·²åˆªé™¤")
        else:
            msg.append("å¯¦é«”æª”æ¡ˆä¸å­˜åœ¨")

        return f"ğŸ—‘ï¸ {filename}: " + "ï¼Œ".join(msg)

    except Exception as e:
        return f"âŒ ç§»é™¤å¤±æ•—ï¼š{str(e)}"

def reset_vector_store():
    """æ¸…ç©ºçŸ¥è­˜åº«èˆ‡ä¸Šå‚³ç›®éŒ„"""
    global vector_store
    logs = []
    
    try:
        # 1. æ¸…ç©º ChromaDB Collection
        try:
            vector_store.delete_collection()
            logs.append("Collection å·²é‡ç½®")
        except:
            pass

        # é‡æ–°åˆå§‹åŒ– Vector Store
        vector_store = Chroma(
            collection_name="smart_deck_docs",
            embedding_function=embeddings,
            persist_directory=PERSIST_DIRECTORY
        )
        
        # 2. [ä¿®æ”¹] ç›´æ¥æ¸…ç©º uploads è³‡æ–™å¤¾
        # é€™æ¨£åšæ›´ä¹¾æ·¨ï¼Œä¸éœ€è¦ç”¨ glob å»éæ¿¾å‰¯æª”å
        if os.path.exists(Config.UPLOAD_DIR):
            # åˆªé™¤æ•´å€‹è³‡æ–™å¤¾
            shutil.rmtree(Config.UPLOAD_DIR)
            # é¦¬ä¸Šé‡å»ºä¸€å€‹ç©ºçš„
            os.makedirs(Config.UPLOAD_DIR)
            logs.append("Uploads ç›®éŒ„å·²æ¸…ç©º")
        
        return "âœ… é‡ç½®å®Œæˆ: " + "ï¼Œ".join(logs)

    except Exception as e:
        return f"âŒ é‡ç½®å¤±æ•—: {str(e)}"

def query_knowledge_base(query: str):
    try:
        results = vector_store.similarity_search(query, k=4)
        if not results:
            return "çŸ¥è­˜åº«ä¸­æ‰¾ä¸åˆ°ç›¸é—œè³‡è¨Šã€‚"
        return "\n\n".join([f"---ç‰‡æ®µ---\n{doc.page_content}" for doc in results])
    except Exception as e:
        return f"æœå°‹å¤±æ•—ï¼š{str(e)}"

# Tool å®šç¾© 
rag_tool = Tool(
    name="read_knowledge_base",
    description="è®€å–å·²ä¸Šå‚³çš„æ–‡ä»¶ã€‚ç”¨æ–¼æŸ¥è©¢å…§éƒ¨è³‡æ–™ã€‚",
    func=query_knowledge_base
)